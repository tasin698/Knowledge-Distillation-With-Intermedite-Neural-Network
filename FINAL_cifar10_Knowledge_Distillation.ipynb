{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2z5Mthy1bnz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST2PwKukPELl",
        "outputId": "d12e3b69-4f30-42de-d93a-8b8974cf05bb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 31357336.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Epoch 1, Loss: 1.597, Test Accuracy: 46.860 %\n",
            "Epoch 2, Loss: 1.116, Test Accuracy: 62.940 %\n",
            "Epoch 3, Loss: 0.930, Test Accuracy: 62.430 %\n",
            "Epoch 4, Loss: 0.805, Test Accuracy: 68.720 %\n",
            "Epoch 5, Loss: 0.726, Test Accuracy: 69.620 %\n",
            "Epoch 6, Loss: 0.673, Test Accuracy: 71.570 %\n",
            "Epoch 7, Loss: 0.648, Test Accuracy: 73.570 %\n",
            "Epoch 8, Loss: 0.619, Test Accuracy: 71.430 %\n",
            "Epoch 9, Loss: 0.604, Test Accuracy: 72.740 %\n",
            "Epoch 10, Loss: 0.588, Test Accuracy: 77.090 %\n",
            "Epoch 11, Loss: 0.576, Test Accuracy: 70.140 %\n",
            "Epoch 12, Loss: 0.564, Test Accuracy: 73.970 %\n",
            "Epoch 13, Loss: 0.555, Test Accuracy: 76.020 %\n",
            "Epoch 14, Loss: 0.551, Test Accuracy: 76.890 %\n",
            "Epoch 15, Loss: 0.544, Test Accuracy: 74.760 %\n",
            "Epoch 16, Loss: 0.544, Test Accuracy: 76.850 %\n",
            "Epoch 17, Loss: 0.537, Test Accuracy: 79.170 %\n",
            "Epoch 18, Loss: 0.529, Test Accuracy: 80.870 %\n",
            "Epoch 19, Loss: 0.524, Test Accuracy: 77.020 %\n",
            "Epoch 20, Loss: 0.517, Test Accuracy: 76.460 %\n",
            "Epoch 21, Loss: 0.521, Test Accuracy: 78.980 %\n",
            "Epoch 22, Loss: 0.511, Test Accuracy: 79.500 %\n",
            "Epoch 23, Loss: 0.506, Test Accuracy: 77.470 %\n",
            "Epoch 24, Loss: 0.505, Test Accuracy: 76.220 %\n",
            "Epoch 25, Loss: 0.502, Test Accuracy: 70.110 %\n",
            "Epoch 26, Loss: 0.503, Test Accuracy: 79.920 %\n",
            "Epoch 27, Loss: 0.499, Test Accuracy: 78.270 %\n",
            "Epoch 28, Loss: 0.505, Test Accuracy: 77.890 %\n",
            "Epoch 29, Loss: 0.495, Test Accuracy: 72.540 %\n",
            "Epoch 30, Loss: 0.501, Test Accuracy: 75.140 %\n",
            "Epoch 31, Loss: 0.492, Test Accuracy: 76.040 %\n",
            "Epoch 32, Loss: 0.484, Test Accuracy: 78.550 %\n",
            "Epoch 33, Loss: 0.486, Test Accuracy: 80.660 %\n",
            "Epoch 34, Loss: 0.488, Test Accuracy: 78.430 %\n",
            "Epoch 35, Loss: 0.479, Test Accuracy: 69.860 %\n",
            "Epoch 36, Loss: 0.487, Test Accuracy: 74.280 %\n",
            "Epoch 37, Loss: 0.485, Test Accuracy: 80.490 %\n",
            "Epoch 38, Loss: 0.481, Test Accuracy: 79.650 %\n",
            "Epoch 39, Loss: 0.478, Test Accuracy: 80.320 %\n",
            "Epoch 40, Loss: 0.480, Test Accuracy: 79.190 %\n",
            "Epoch 41, Loss: 0.480, Test Accuracy: 80.920 %\n",
            "Epoch 42, Loss: 0.475, Test Accuracy: 76.860 %\n",
            "Epoch 43, Loss: 0.477, Test Accuracy: 77.580 %\n",
            "Epoch 44, Loss: 0.474, Test Accuracy: 79.030 %\n",
            "Epoch 45, Loss: 0.479, Test Accuracy: 74.850 %\n",
            "Epoch 46, Loss: 0.472, Test Accuracy: 78.890 %\n",
            "Epoch 47, Loss: 0.472, Test Accuracy: 75.130 %\n",
            "Epoch 48, Loss: 0.479, Test Accuracy: 77.440 %\n",
            "Epoch 49, Loss: 0.472, Test Accuracy: 79.800 %\n",
            "Epoch 50, Loss: 0.470, Test Accuracy: 81.110 %\n",
            "Epoch 51, Loss: 0.470, Test Accuracy: 79.760 %\n",
            "Epoch 52, Loss: 0.471, Test Accuracy: 75.940 %\n",
            "Epoch 53, Loss: 0.469, Test Accuracy: 76.400 %\n",
            "Epoch 54, Loss: 0.469, Test Accuracy: 79.270 %\n",
            "Epoch 55, Loss: 0.465, Test Accuracy: 73.050 %\n",
            "Epoch 56, Loss: 0.466, Test Accuracy: 74.940 %\n",
            "Epoch 57, Loss: 0.466, Test Accuracy: 75.140 %\n",
            "Epoch 58, Loss: 0.465, Test Accuracy: 75.850 %\n",
            "Epoch 59, Loss: 0.465, Test Accuracy: 69.990 %\n",
            "Epoch 60, Loss: 0.462, Test Accuracy: 78.490 %\n",
            "Epoch 61, Loss: 0.461, Test Accuracy: 78.820 %\n",
            "Epoch 62, Loss: 0.461, Test Accuracy: 80.800 %\n",
            "Epoch 63, Loss: 0.464, Test Accuracy: 80.820 %\n",
            "Epoch 64, Loss: 0.462, Test Accuracy: 69.770 %\n",
            "Epoch 65, Loss: 0.461, Test Accuracy: 76.000 %\n"
          ]
        }
      ],
      "source": [
        "#Train resnet32 teacher model for cifar10\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Define transforms for data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
        "])\n",
        "\n",
        "# Load CIFAR10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = nn.ReLU()(out)\n",
        "        return out\n",
        "\n",
        "class ResNet32(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet32, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * BasicBlock.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = nn.AvgPool2d(8)(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "#Define training parameters\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "teacher = ResNet32().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(teacher.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(65):\n",
        "    teacher.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = teacher(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    teacher.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = teacher(images).to(device)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print('Epoch %d, Loss: %.3f, Test Accuracy: %.3f %%' % (epoch+1, running_loss/len(trainloader), 100*correct/total))\n",
        "\n",
        "torch.save(teacher.state_dict(), '/content/drive/MyDrive/Teacher32_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKUtBimY11n7"
      },
      "outputs": [],
      "source": [
        "# Defining the teacher and TA models\n",
        "\n",
        "class BasicBlock_32(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock_32, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = nn.ReLU()(out)\n",
        "        return out\n",
        "\n",
        "class ResNet32(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet32, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock_32(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * BasicBlock_32.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = nn.AvgPool2d(8)(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.stride = stride\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet20, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(BasicBlock, 16, 3, stride=1)\n",
        "        self.layer2 = self.make_layer(BasicBlock, 32, 3, stride=2)\n",
        "        self.layer3 = self.make_layer(BasicBlock, 64, 3, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbLk4Bwr19N2",
        "outputId": "8a972c2b-2e35-41b0-917f-3750a1f7e597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 93087032.51it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Load the datasets\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
        "])\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "cifar10_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# Calculate the lengths of the training and testing datasets\n",
        "train_length = int(len(cifar10_dataset) * 0.8)\n",
        "test_length = len(cifar10_dataset) - train_length\n",
        "\n",
        "# Split the CIFAR10 dataset into training and testing datasets\n",
        "train_dataset, test_dataset = random_split(cifar10_dataset, [train_length, test_length])\n",
        "\n",
        "# Apply the test_transform to the test_dataset\n",
        "test_dataset = test_dataset.dataset\n",
        "test_dataset.transform = test_transform\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUpiqEM82EFu",
        "outputId": "1015eb85-26e7-4b44-d03f-1a02c7073907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function, optimizer, and hyperparameters\n",
        "teacher = ResNet32()\n",
        "TA=ResNet20()\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "M=ResNet(block, num_blocks)\n",
        "TA=M.ResNet20()\n",
        "student=M.Resnet8()\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load pre-trained teacher model weights\n",
        "#teacher.resnet50.load_state_dict(torch.load('/content/drive/MyDrive/resnet50_cifar10.pth'), False)\n",
        "teacher.load_state_dict(torch.load('/content/drive/MyDrive/Teacher32_weights.pth'), False)\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.SGD(TA.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ennU7wnL2RBr",
        "outputId": "996e172f-1432-4f2e-f7e2-1ebc395dee1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/40], Train Loss: 3.8500, Train Acc: 37.95%, Test Loss: 2.0088, Test Acc: 47.20%\n",
            "Epoch [2/40], Train Loss: 1.8279, Train Acc: 58.86%, Test Loss: 2.0090, Test Acc: 51.79%\n",
            "Epoch [3/40], Train Loss: 1.1542, Train Acc: 66.69%, Test Loss: 0.8964, Test Acc: 71.28%\n",
            "Epoch [4/40], Train Loss: 0.8615, Train Acc: 70.16%, Test Loss: 1.2325, Test Acc: 65.73%\n",
            "Epoch [5/40], Train Loss: 0.7031, Train Acc: 71.94%, Test Loss: 1.0002, Test Acc: 69.37%\n",
            "Epoch [6/40], Train Loss: 0.5994, Train Acc: 73.59%, Test Loss: 0.8580, Test Acc: 73.20%\n",
            "Epoch [7/40], Train Loss: 0.5387, Train Acc: 73.95%, Test Loss: 0.8401, Test Acc: 73.16%\n",
            "Epoch [8/40], Train Loss: 0.4815, Train Acc: 74.39%, Test Loss: 0.7369, Test Acc: 75.64%\n",
            "Epoch [9/40], Train Loss: 0.4548, Train Acc: 74.91%, Test Loss: 0.7956, Test Acc: 74.84%\n",
            "Epoch [10/40], Train Loss: 0.4122, Train Acc: 75.19%, Test Loss: 0.8029, Test Acc: 75.01%\n",
            "Epoch [11/40], Train Loss: 0.4010, Train Acc: 75.32%, Test Loss: 0.9584, Test Acc: 71.82%\n",
            "Epoch [12/40], Train Loss: 0.3712, Train Acc: 75.73%, Test Loss: 0.8587, Test Acc: 73.06%\n",
            "Epoch [13/40], Train Loss: 0.3450, Train Acc: 75.90%, Test Loss: 0.8697, Test Acc: 72.92%\n",
            "Epoch [14/40], Train Loss: 0.3338, Train Acc: 75.89%, Test Loss: 0.7983, Test Acc: 75.23%\n",
            "Epoch [15/40], Train Loss: 0.3186, Train Acc: 76.26%, Test Loss: 0.7672, Test Acc: 75.58%\n",
            "Epoch [16/40], Train Loss: 0.3075, Train Acc: 76.43%, Test Loss: 0.8779, Test Acc: 73.18%\n",
            "Epoch [17/40], Train Loss: 0.2994, Train Acc: 76.41%, Test Loss: 0.8266, Test Acc: 73.87%\n",
            "Epoch [18/40], Train Loss: 0.2834, Train Acc: 76.49%, Test Loss: 0.6830, Test Acc: 78.13%\n",
            "Epoch [19/40], Train Loss: 0.2831, Train Acc: 76.58%, Test Loss: 0.6301, Test Acc: 79.33%\n",
            "Epoch [20/40], Train Loss: 0.2752, Train Acc: 76.50%, Test Loss: 0.7423, Test Acc: 76.42%\n",
            "Epoch [21/40], Train Loss: 0.2583, Train Acc: 76.82%, Test Loss: 0.8024, Test Acc: 74.92%\n",
            "Epoch [22/40], Train Loss: 0.2523, Train Acc: 76.78%, Test Loss: 0.7272, Test Acc: 76.42%\n",
            "Epoch [23/40], Train Loss: 0.2529, Train Acc: 76.99%, Test Loss: 0.7693, Test Acc: 75.56%\n",
            "Epoch [24/40], Train Loss: 0.2483, Train Acc: 76.71%, Test Loss: 0.6307, Test Acc: 79.05%\n",
            "Epoch [25/40], Train Loss: 0.2399, Train Acc: 76.92%, Test Loss: 0.7792, Test Acc: 74.67%\n",
            "Epoch [26/40], Train Loss: 0.2390, Train Acc: 76.88%, Test Loss: 0.7333, Test Acc: 76.65%\n",
            "Epoch [27/40], Train Loss: 0.2328, Train Acc: 76.79%, Test Loss: 0.7199, Test Acc: 76.84%\n",
            "Epoch [28/40], Train Loss: 0.2226, Train Acc: 76.78%, Test Loss: 0.7635, Test Acc: 76.04%\n",
            "Epoch [29/40], Train Loss: 0.2265, Train Acc: 76.92%, Test Loss: 0.7930, Test Acc: 74.42%\n",
            "Epoch [30/40], Train Loss: 0.2221, Train Acc: 76.88%, Test Loss: 0.7811, Test Acc: 75.02%\n",
            "Epoch [31/40], Train Loss: 0.1673, Train Acc: 77.48%, Test Loss: 0.6889, Test Acc: 77.69%\n",
            "Epoch [32/40], Train Loss: 0.1613, Train Acc: 77.41%, Test Loss: 0.6982, Test Acc: 77.60%\n",
            "Epoch [33/40], Train Loss: 0.1562, Train Acc: 77.58%, Test Loss: 0.7082, Test Acc: 77.38%\n",
            "Epoch [34/40], Train Loss: 0.1509, Train Acc: 77.56%, Test Loss: 0.7000, Test Acc: 77.61%\n",
            "Epoch [35/40], Train Loss: 0.1501, Train Acc: 77.64%, Test Loss: 0.6928, Test Acc: 77.64%\n",
            "Epoch [36/40], Train Loss: 0.1499, Train Acc: 77.49%, Test Loss: 0.7075, Test Acc: 77.23%\n",
            "Epoch [37/40], Train Loss: 0.1505, Train Acc: 77.58%, Test Loss: 0.6989, Test Acc: 77.50%\n",
            "Epoch [38/40], Train Loss: 0.1488, Train Acc: 77.56%, Test Loss: 0.7145, Test Acc: 77.18%\n",
            "Epoch [39/40], Train Loss: 0.1442, Train Acc: 77.47%, Test Loss: 0.7062, Test Acc: 77.39%\n",
            "Epoch [40/40], Train Loss: 0.1469, Train Acc: 77.58%, Test Loss: 0.7144, Test Acc: 77.20%\n"
          ]
        }
      ],
      "source": [
        "# Train the TA model using knowledge distillation\n",
        "\n",
        "temperature = 3.3\n",
        "num_epochs = 40\n",
        "\n",
        "\n",
        "teacher.eval()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    TA.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for teacher and student models\n",
        "        with torch.no_grad():\n",
        "            teacher_output = teacher(images)\n",
        "        TA_output = TA(images)\n",
        "\n",
        "        # Apply temperature scaling to logits\n",
        "        teacher_output = teacher_output / temperature\n",
        "        TA_output = TA_output / temperature\n",
        "\n",
        "        # Calculate the distillation loss\n",
        "        loss = criterion(nn.functional.log_softmax(TA_output, dim=1), nn.functional.softmax(teacher_output, dim=1))*(temperature**2)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(TA_output.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate the student model on the test set\n",
        "    TA.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            TA_output = TA(images)\n",
        "            loss = nn.functional.cross_entropy(TA_output, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(TA_output.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print epoch number, training loss, training accuracy, test loss, and test accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, train_loss/len(train_dataset), 100*train_correct/len(train_dataset),\n",
        "                  test_loss/len(test_dataset), 100*test_correct/len(test_dataset)))\n",
        "\n",
        "# Save the student model weights\n",
        "torch.save(TA.state_dict(), '/content/drive/MyDrive/TA_ResNet20_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNbET1KGB47y",
        "outputId": "a4ec8a49-a86f-4a96-bbf8-2f63b9fb383b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load trained TA model weights\n",
        "TA.load_state_dict(torch.load('/content/drive/MyDrive/TA_ResNet20_weights.pth'), False)\n",
        "\n",
        "TA.eval()\n",
        "\n",
        "class ResNet8(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet8, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(BasicBlock, 16, 1, stride=1)\n",
        "        self.layer2 = self.make_layer(BasicBlock, 32, 1, stride=2)\n",
        "        self.layer3 = self.make_layer(BasicBlock, 64, 1, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "student = ResNet8()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z31gBHFNsJi",
        "outputId": "3a33711a-3594-4279-b650-881d31e5090e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch [1/40], Train Loss: 3.8130, Train Acc: 36.85%, Test Loss: 2.1777, Test Acc: 39.68%\n",
            "Epoch [2/40], Train Loss: 1.9900, Train Acc: 55.54%, Test Loss: 1.5895, Test Acc: 52.48%\n",
            "Epoch [3/40], Train Loss: 1.3029, Train Acc: 62.96%, Test Loss: 1.1428, Test Acc: 64.46%\n",
            "Epoch [4/40], Train Loss: 1.0240, Train Acc: 66.47%, Test Loss: 0.9865, Test Acc: 68.52%\n",
            "Epoch [5/40], Train Loss: 0.8850, Train Acc: 68.00%, Test Loss: 0.8778, Test Acc: 71.50%\n",
            "Epoch [6/40], Train Loss: 0.7480, Train Acc: 69.98%, Test Loss: 0.8265, Test Acc: 72.14%\n",
            "Epoch [7/40], Train Loss: 0.6973, Train Acc: 70.38%, Test Loss: 1.0993, Test Acc: 66.70%\n",
            "Epoch [8/40], Train Loss: 0.6545, Train Acc: 71.06%, Test Loss: 0.8087, Test Acc: 73.51%\n",
            "Epoch [9/40], Train Loss: 0.6275, Train Acc: 71.12%, Test Loss: 1.3002, Test Acc: 63.15%\n",
            "Epoch [10/40], Train Loss: 0.6197, Train Acc: 71.22%, Test Loss: 0.7420, Test Acc: 75.22%\n",
            "Epoch [11/40], Train Loss: 0.5894, Train Acc: 71.60%, Test Loss: 1.0705, Test Acc: 68.02%\n",
            "Epoch [12/40], Train Loss: 0.5825, Train Acc: 71.92%, Test Loss: 1.0660, Test Acc: 65.78%\n",
            "Epoch [13/40], Train Loss: 0.5690, Train Acc: 71.80%, Test Loss: 1.1766, Test Acc: 65.69%\n",
            "Epoch [14/40], Train Loss: 0.5779, Train Acc: 71.92%, Test Loss: 0.8494, Test Acc: 72.93%\n",
            "Epoch [15/40], Train Loss: 0.5539, Train Acc: 72.18%, Test Loss: 0.8499, Test Acc: 72.73%\n",
            "Epoch [16/40], Train Loss: 0.5512, Train Acc: 72.17%, Test Loss: 1.3305, Test Acc: 61.51%\n",
            "Epoch [17/40], Train Loss: 0.5425, Train Acc: 72.14%, Test Loss: 0.9003, Test Acc: 70.94%\n",
            "Epoch [18/40], Train Loss: 0.5308, Train Acc: 72.31%, Test Loss: 0.9766, Test Acc: 68.86%\n",
            "Epoch [19/40], Train Loss: 0.5247, Train Acc: 72.47%, Test Loss: 0.8891, Test Acc: 70.50%\n",
            "Epoch [20/40], Train Loss: 0.5252, Train Acc: 72.57%, Test Loss: 0.8135, Test Acc: 74.22%\n",
            "Epoch [21/40], Train Loss: 0.5252, Train Acc: 72.63%, Test Loss: 0.9066, Test Acc: 71.29%\n",
            "Epoch [22/40], Train Loss: 0.5186, Train Acc: 72.50%, Test Loss: 1.0992, Test Acc: 68.65%\n",
            "Epoch [23/40], Train Loss: 0.5124, Train Acc: 72.60%, Test Loss: 0.9618, Test Acc: 69.92%\n",
            "Epoch [24/40], Train Loss: 0.5223, Train Acc: 72.61%, Test Loss: 1.0083, Test Acc: 69.44%\n",
            "Epoch [25/40], Train Loss: 0.5130, Train Acc: 72.68%, Test Loss: 1.0121, Test Acc: 68.81%\n",
            "Epoch [26/40], Train Loss: 0.5031, Train Acc: 72.60%, Test Loss: 1.0140, Test Acc: 69.80%\n",
            "Epoch [27/40], Train Loss: 0.5141, Train Acc: 72.76%, Test Loss: 1.2237, Test Acc: 64.86%\n",
            "Epoch [28/40], Train Loss: 0.5045, Train Acc: 72.84%, Test Loss: 0.7261, Test Acc: 75.66%\n",
            "Epoch [29/40], Train Loss: 0.4970, Train Acc: 72.79%, Test Loss: 0.8709, Test Acc: 71.63%\n",
            "Epoch [30/40], Train Loss: 0.5277, Train Acc: 72.60%, Test Loss: 0.9871, Test Acc: 69.64%\n",
            "Epoch [31/40], Train Loss: 0.3269, Train Acc: 74.58%, Test Loss: 0.7477, Test Acc: 75.72%\n",
            "Epoch [32/40], Train Loss: 0.2978, Train Acc: 74.90%, Test Loss: 0.7565, Test Acc: 75.62%\n",
            "Epoch [33/40], Train Loss: 0.2875, Train Acc: 74.97%, Test Loss: 0.7918, Test Acc: 74.66%\n",
            "Epoch [34/40], Train Loss: 0.2830, Train Acc: 75.05%, Test Loss: 0.7306, Test Acc: 76.09%\n",
            "Epoch [35/40], Train Loss: 0.2732, Train Acc: 75.04%, Test Loss: 0.7910, Test Acc: 74.77%\n",
            "Epoch [36/40], Train Loss: 0.2705, Train Acc: 75.33%, Test Loss: 0.7444, Test Acc: 75.59%\n",
            "Epoch [37/40], Train Loss: 0.2681, Train Acc: 75.32%, Test Loss: 0.7698, Test Acc: 75.39%\n",
            "Epoch [38/40], Train Loss: 0.2661, Train Acc: 75.26%, Test Loss: 0.7076, Test Acc: 76.85%\n",
            "Epoch [39/40], Train Loss: 0.2669, Train Acc: 75.11%, Test Loss: 0.7375, Test Acc: 76.17%\n",
            "Epoch [40/40], Train Loss: 0.2611, Train Acc: 75.20%, Test Loss: 0.7524, Test Acc: 75.82%\n"
          ]
        }
      ],
      "source": [
        "# Train the student model using knowledge distillation without replacing the Fully Connected Layer(FC)\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "temperature = 3.3\n",
        "num_epochs = 40\n",
        "alpha = 0.5 # weight for the negative log likelihood loss\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for teacher and student models\n",
        "        with torch.no_grad():\n",
        "           TA_output = TA(images)\n",
        "        student_output = student(images)\n",
        "\n",
        "        # Apply temperature scaling to logits\n",
        "        TA_output = TA_output / temperature\n",
        "        student_output = student_output / temperature\n",
        "\n",
        "        # Calculate the distillation loss\n",
        "        distillation_loss = criterion(nn.functional.log_softmax(student_output, dim=1), nn.functional.softmax(TA_output, dim=1))*(temperature**2)\n",
        "\n",
        "         # Calculate the negative log likelihood loss\n",
        "        nll_loss = nn.functional.nll_loss(nn.functional.log_softmax(TA_output, dim=1), labels)\n",
        "\n",
        "        # Combine the two losses using the weight alpha\n",
        "        loss = alpha * nll_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(student_output.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate the student model on the test set\n",
        "    student.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            student_output = student(images)\n",
        "            loss = nn.functional.cross_entropy(student_output, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(student_output.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print epoch number, training loss, training accuracy, test loss, and test accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, train_loss/len(train_dataset), 100*train_correct/len(train_dataset),\n",
        "                  test_loss/len(test_dataset), 100*test_correct/len(test_dataset)))\n",
        "\n",
        "\n",
        "# Save the student model weights\n",
        "torch.save(student.state_dict(), '/content/drive/MyDrive/student_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WBnY3Ntrhmd",
        "outputId": "2a6c7b10-8874-4cf0-9730-85bd419aa02b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Replacing student's fc by teacher's fc\n",
        "teacher.eval()\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Load trained TA model weights\n",
        "TA.load_state_dict(torch.load('/content/drive/MyDrive/TA_ResNet20_weights.pth'), False)\n",
        "TA.eval()\n",
        "\n",
        "class ResNet8(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet8, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(BasicBlock, 16, 1, stride=1)\n",
        "        self.layer2 = self.make_layer(BasicBlock, 32, 1, stride=2)\n",
        "        self.layer3 = self.make_layer(BasicBlock, 64, 1, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "\n",
        "        #Replace the student's fully connected network by teacher's liniar layer(64,10)\n",
        "        student_fc_layer=nn.Linear(64, 10)\n",
        "        student_fc_layer.weight.data = teacher.fc.weight.data\n",
        "        student_fc_layer.bias.data = teacher.fc.bias.data\n",
        "        self.fc = student_fc_layer\n",
        "\n",
        "\n",
        "        #self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "student = ResNet8()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train after replacing fc of student and loss*temperature**2 including the NLL\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load trained TA model weights\n",
        "TA.load_state_dict(torch.load('/content/drive/MyDrive/TA_ResNet20_weight.pth'), False)\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "temperature = 3.16\n",
        "num_epochs = 40\n",
        "alpha = 0.5\n",
        "# TA.eval()\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for teacher and student models\n",
        "        with torch.no_grad():\n",
        "           TA_output = TA(images)\n",
        "        student_output = student(images)\n",
        "\n",
        "        # Apply temperature scaling to logits\n",
        "        TA_output = TA_output / temperature\n",
        "        student_output = student_output / temperature\n",
        "\n",
        "        # Calculate the distillation loss\n",
        "        distillation_loss = criterion(nn.functional.log_softmax(student_output, dim=1), nn.functional.softmax(TA_output, dim=1))*(temperature**2)\n",
        "\n",
        "        # Calculate the negative log likelihood loss\n",
        "        nll_loss = nn.functional.nll_loss(nn.functional.log_softmax(TA_output, dim=1), labels)\n",
        "\n",
        "        # Combine the two losses using the weight alpha\n",
        "        loss = alpha * nll_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(student_output.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate the student model on the test set\n",
        "    student.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            student_output = student(images)\n",
        "            loss = nn.functional.cross_entropy(student_output, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(student_output.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print epoch number, training loss, training accuracy, test loss, and test accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, train_loss/len(train_dataset), 100*train_correct/len(train_dataset),\n",
        "                  test_loss/len(test_dataset), 100*test_correct/len(test_dataset)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the student model weights\n",
        "torch.save(student.state_dict(), '/content/drive/MyDrive/student_weights.pth')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bi_D4RLc6OIo",
        "outputId": "4484d74e-0df0-444d-caa4-edf1d2291da7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Epoch [1/40], Train Loss: 3.9222, Train Acc: 35.15%, Test Loss: 1.9644, Test Acc: 43.66%\n",
            "Epoch [2/40], Train Loss: 2.2709, Train Acc: 52.52%, Test Loss: 1.3271, Test Acc: 57.64%\n",
            "Epoch [3/40], Train Loss: 1.4431, Train Acc: 61.16%, Test Loss: 1.0183, Test Acc: 66.31%\n",
            "Epoch [4/40], Train Loss: 1.0658, Train Acc: 65.51%, Test Loss: 1.2976, Test Acc: 63.54%\n",
            "Epoch [5/40], Train Loss: 0.8624, Train Acc: 68.13%, Test Loss: 1.0951, Test Acc: 67.26%\n",
            "Epoch [6/40], Train Loss: 0.7765, Train Acc: 69.19%, Test Loss: 1.0218, Test Acc: 66.55%\n",
            "Epoch [7/40], Train Loss: 0.7023, Train Acc: 70.16%, Test Loss: 0.8953, Test Acc: 71.17%\n",
            "Epoch [8/40], Train Loss: 0.6618, Train Acc: 70.59%, Test Loss: 0.9419, Test Acc: 69.03%\n",
            "Epoch [9/40], Train Loss: 0.6407, Train Acc: 70.95%, Test Loss: 0.8062, Test Acc: 72.91%\n",
            "Epoch [10/40], Train Loss: 0.5980, Train Acc: 71.81%, Test Loss: 1.1242, Test Acc: 67.19%\n",
            "Epoch [11/40], Train Loss: 0.5997, Train Acc: 71.62%, Test Loss: 1.4290, Test Acc: 58.98%\n",
            "Epoch [12/40], Train Loss: 0.5644, Train Acc: 71.70%, Test Loss: 0.8884, Test Acc: 72.45%\n",
            "Epoch [13/40], Train Loss: 0.5859, Train Acc: 72.04%, Test Loss: 0.7853, Test Acc: 74.45%\n",
            "Epoch [14/40], Train Loss: 0.5565, Train Acc: 72.17%, Test Loss: 0.7782, Test Acc: 74.46%\n",
            "Epoch [15/40], Train Loss: 0.5633, Train Acc: 72.08%, Test Loss: 1.2886, Test Acc: 64.18%\n",
            "Epoch [16/40], Train Loss: 0.5617, Train Acc: 71.98%, Test Loss: 1.0676, Test Acc: 68.52%\n",
            "Epoch [17/40], Train Loss: 0.5488, Train Acc: 72.31%, Test Loss: 0.8111, Test Acc: 73.82%\n",
            "Epoch [18/40], Train Loss: 0.5300, Train Acc: 72.53%, Test Loss: 0.8457, Test Acc: 71.89%\n",
            "Epoch [19/40], Train Loss: 0.5396, Train Acc: 72.40%, Test Loss: 1.1463, Test Acc: 65.16%\n",
            "Epoch [20/40], Train Loss: 0.5473, Train Acc: 72.17%, Test Loss: 1.0671, Test Acc: 68.32%\n",
            "Epoch [21/40], Train Loss: 0.5155, Train Acc: 72.78%, Test Loss: 1.1313, Test Acc: 67.67%\n",
            "Epoch [22/40], Train Loss: 0.5174, Train Acc: 72.57%, Test Loss: 0.7670, Test Acc: 74.25%\n",
            "Epoch [23/40], Train Loss: 0.5127, Train Acc: 72.70%, Test Loss: 0.9221, Test Acc: 71.37%\n",
            "Epoch [24/40], Train Loss: 0.5173, Train Acc: 72.77%, Test Loss: 1.2756, Test Acc: 63.22%\n",
            "Epoch [25/40], Train Loss: 0.5132, Train Acc: 72.59%, Test Loss: 0.9687, Test Acc: 69.76%\n",
            "Epoch [26/40], Train Loss: 0.5272, Train Acc: 72.62%, Test Loss: 0.8135, Test Acc: 73.31%\n",
            "Epoch [27/40], Train Loss: 0.4976, Train Acc: 72.79%, Test Loss: 1.0023, Test Acc: 69.65%\n",
            "Epoch [28/40], Train Loss: 0.4995, Train Acc: 73.01%, Test Loss: 1.2854, Test Acc: 64.79%\n",
            "Epoch [29/40], Train Loss: 0.5001, Train Acc: 73.08%, Test Loss: 0.9646, Test Acc: 69.12%\n",
            "Epoch [30/40], Train Loss: 0.5075, Train Acc: 72.79%, Test Loss: 1.0114, Test Acc: 69.49%\n",
            "Epoch [31/40], Train Loss: 0.3276, Train Acc: 74.67%, Test Loss: 0.7561, Test Acc: 75.31%\n",
            "Epoch [32/40], Train Loss: 0.2942, Train Acc: 75.10%, Test Loss: 0.7235, Test Acc: 76.20%\n",
            "Epoch [33/40], Train Loss: 0.2853, Train Acc: 75.13%, Test Loss: 0.7909, Test Acc: 74.85%\n",
            "Epoch [34/40], Train Loss: 0.2775, Train Acc: 75.27%, Test Loss: 0.7517, Test Acc: 75.62%\n",
            "Epoch [35/40], Train Loss: 0.2751, Train Acc: 75.25%, Test Loss: 0.7476, Test Acc: 75.66%\n",
            "Epoch [36/40], Train Loss: 0.2727, Train Acc: 75.15%, Test Loss: 0.7562, Test Acc: 75.42%\n",
            "Epoch [37/40], Train Loss: 0.2645, Train Acc: 75.43%, Test Loss: 0.7238, Test Acc: 76.42%\n",
            "Epoch [38/40], Train Loss: 0.2639, Train Acc: 75.54%, Test Loss: 0.8087, Test Acc: 74.20%\n",
            "Epoch [39/40], Train Loss: 0.2620, Train Acc: 75.46%, Test Loss: 0.8234, Test Acc: 74.10%\n",
            "Epoch [40/40], Train Loss: 0.2573, Train Acc: 75.46%, Test Loss: 0.6954, Test Acc: 76.94%\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}