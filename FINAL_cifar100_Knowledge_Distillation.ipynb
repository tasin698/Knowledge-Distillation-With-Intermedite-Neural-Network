{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2z5Mthy1bnz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHEmJlycJgS5",
        "outputId": "c1f9f3e2-0ad9-4b13-abae-8913443323a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[Epoch 1] loss: 3.977\n",
            "[Epoch 1] test accuracy: 11.350%\n",
            "[Epoch 2] loss: 3.442\n",
            "[Epoch 2] test accuracy: 18.540%\n",
            "[Epoch 3] loss: 2.962\n",
            "[Epoch 3] test accuracy: 26.200%\n",
            "[Epoch 4] loss: 2.648\n",
            "[Epoch 4] test accuracy: 28.370%\n",
            "[Epoch 5] loss: 2.435\n",
            "[Epoch 5] test accuracy: 33.500%\n",
            "[Epoch 6] loss: 2.281\n",
            "[Epoch 6] test accuracy: 32.480%\n",
            "[Epoch 7] loss: 2.188\n",
            "[Epoch 7] test accuracy: 36.720%\n",
            "[Epoch 8] loss: 2.098\n",
            "[Epoch 8] test accuracy: 39.020%\n",
            "[Epoch 9] loss: 2.033\n",
            "[Epoch 9] test accuracy: 41.560%\n",
            "[Epoch 10] loss: 1.981\n",
            "[Epoch 10] test accuracy: 39.150%\n",
            "[Epoch 11] loss: 1.943\n",
            "[Epoch 11] test accuracy: 36.840%\n",
            "[Epoch 12] loss: 1.914\n",
            "[Epoch 12] test accuracy: 39.330%\n",
            "[Epoch 13] loss: 1.888\n",
            "[Epoch 13] test accuracy: 44.300%\n",
            "[Epoch 14] loss: 1.856\n",
            "[Epoch 14] test accuracy: 41.010%\n",
            "[Epoch 15] loss: 1.836\n",
            "[Epoch 15] test accuracy: 36.980%\n",
            "[Epoch 16] loss: 1.825\n",
            "[Epoch 16] test accuracy: 47.280%\n",
            "[Epoch 17] loss: 1.802\n",
            "[Epoch 17] test accuracy: 42.310%\n",
            "[Epoch 18] loss: 1.789\n",
            "[Epoch 18] test accuracy: 41.620%\n",
            "[Epoch 19] loss: 1.782\n",
            "[Epoch 19] test accuracy: 42.250%\n",
            "[Epoch 20] loss: 1.769\n",
            "[Epoch 20] test accuracy: 42.580%\n",
            "[Epoch 21] loss: 1.752\n",
            "[Epoch 21] test accuracy: 43.530%\n",
            "[Epoch 22] loss: 1.747\n",
            "[Epoch 22] test accuracy: 45.500%\n",
            "[Epoch 23] loss: 1.747\n",
            "[Epoch 23] test accuracy: 44.660%\n",
            "[Epoch 24] loss: 1.723\n",
            "[Epoch 24] test accuracy: 44.750%\n",
            "[Epoch 25] loss: 1.729\n",
            "[Epoch 25] test accuracy: 42.190%\n",
            "[Epoch 26] loss: 1.720\n",
            "[Epoch 26] test accuracy: 43.430%\n",
            "[Epoch 27] loss: 1.715\n",
            "[Epoch 27] test accuracy: 46.570%\n",
            "[Epoch 28] loss: 1.703\n",
            "[Epoch 28] test accuracy: 47.520%\n",
            "[Epoch 29] loss: 1.705\n",
            "[Epoch 29] test accuracy: 44.780%\n",
            "[Epoch 30] loss: 1.701\n",
            "[Epoch 30] test accuracy: 41.460%\n",
            "[Epoch 31] loss: 1.694\n",
            "[Epoch 31] test accuracy: 47.250%\n",
            "[Epoch 32] loss: 1.684\n",
            "[Epoch 32] test accuracy: 37.810%\n",
            "[Epoch 33] loss: 1.694\n",
            "[Epoch 33] test accuracy: 47.020%\n",
            "[Epoch 34] loss: 1.679\n",
            "[Epoch 34] test accuracy: 39.640%\n",
            "[Epoch 35] loss: 1.672\n",
            "[Epoch 35] test accuracy: 42.220%\n",
            "[Epoch 36] loss: 1.669\n",
            "[Epoch 36] test accuracy: 47.150%\n",
            "[Epoch 37] loss: 1.668\n",
            "[Epoch 37] test accuracy: 47.710%\n",
            "[Epoch 38] loss: 1.666\n",
            "[Epoch 38] test accuracy: 43.170%\n",
            "[Epoch 39] loss: 1.676\n",
            "[Epoch 39] test accuracy: 37.570%\n",
            "[Epoch 40] loss: 1.673\n",
            "[Epoch 40] test accuracy: 46.300%\n",
            "[Epoch 41] loss: 1.660\n",
            "[Epoch 41] test accuracy: 44.310%\n",
            "[Epoch 42] loss: 1.663\n",
            "[Epoch 42] test accuracy: 40.030%\n",
            "[Epoch 43] loss: 1.660\n",
            "[Epoch 43] test accuracy: 47.010%\n",
            "[Epoch 44] loss: 1.653\n",
            "[Epoch 44] test accuracy: 46.760%\n",
            "[Epoch 45] loss: 1.654\n",
            "[Epoch 45] test accuracy: 44.980%\n",
            "[Epoch 46] loss: 1.649\n",
            "[Epoch 46] test accuracy: 40.000%\n",
            "[Epoch 47] loss: 1.645\n",
            "[Epoch 47] test accuracy: 46.890%\n",
            "[Epoch 48] loss: 1.647\n",
            "[Epoch 48] test accuracy: 46.590%\n",
            "[Epoch 49] loss: 1.643\n",
            "[Epoch 49] test accuracy: 44.800%\n",
            "[Epoch 50] loss: 1.646\n",
            "[Epoch 50] test accuracy: 47.610%\n",
            "[Epoch 51] loss: 1.636\n",
            "[Epoch 51] test accuracy: 46.890%\n",
            "[Epoch 52] loss: 1.643\n",
            "[Epoch 52] test accuracy: 42.360%\n",
            "[Epoch 53] loss: 1.644\n",
            "[Epoch 53] test accuracy: 41.150%\n",
            "[Epoch 54] loss: 1.634\n",
            "[Epoch 54] test accuracy: 43.750%\n",
            "[Epoch 55] loss: 1.644\n",
            "[Epoch 55] test accuracy: 45.530%\n",
            "[Epoch 56] loss: 1.636\n",
            "[Epoch 56] test accuracy: 43.910%\n",
            "[Epoch 57] loss: 1.632\n",
            "[Epoch 57] test accuracy: 47.790%\n",
            "[Epoch 58] loss: 1.636\n",
            "[Epoch 58] test accuracy: 46.650%\n",
            "[Epoch 59] loss: 1.633\n",
            "[Epoch 59] test accuracy: 44.740%\n",
            "[Epoch 60] loss: 1.630\n",
            "[Epoch 60] test accuracy: 51.410%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# Define the transforms for data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, .2565, 0.2761])\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, .2565, 0.2761])\n",
        "])\n",
        "\n",
        "# Load the CIFAR-100 dataset\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = nn.ReLU()(out)\n",
        "        return out\n",
        "\n",
        "class ResNet32(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ResNet32, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * BasicBlock.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = nn.AvgPool2d(8)(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "# Initialize the model and optimizer\n",
        "net = ResNet32()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(60):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('[Epoch %d] loss: %.3f' % (epoch + 1, running_loss / len(trainloader)))\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    print('[Epoch %d] test accuracy: %.3f%%' % (epoch + 1, 100 * correct / total))\n",
        "\n",
        "    torch.save(net.state_dict(), '/content/drive/MyDrive/Teacher32_Cifar100_final_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKUtBimY11n7"
      },
      "outputs": [],
      "source": [
        "# Defining the teacher and student and TA models\n",
        "\n",
        "class BasicBlock_32(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock_32, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = nn.ReLU()(out)\n",
        "        return out\n",
        "\n",
        "class ResNet32(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ResNet32, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(16, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(32, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(64, 2, stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "    def _make_layer(self, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(BasicBlock_32(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * BasicBlock_32.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = nn.ReLU()(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = nn.AvgPool2d(8)(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.stride = stride\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(residual)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet20(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ResNet20, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(BasicBlock, 16, 3, stride=1)\n",
        "        self.layer2 = self.make_layer(BasicBlock, 32, 3, stride=2)\n",
        "        self.layer3 = self.make_layer(BasicBlock, 64, 3, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbLk4Bwr19N2",
        "outputId": "0f99a8f7-2b0e-49d6-8cd8-6338cc030627"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 169001437/169001437 [00:15<00:00, 10624845.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Load the datasets\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
        "])\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "cifar100_dataset = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
        "\n",
        "# Calculate the lengths of the training and testing datasets\n",
        "train_length = int(len(cifar100_dataset) * 0.8)\n",
        "test_length = len(cifar100_dataset) - train_length\n",
        "\n",
        "# Split the CIFAR10 dataset into training and testing datasets\n",
        "train_dataset, test_dataset = random_split(cifar100_dataset, [train_length, test_length])\n",
        "\n",
        "# Apply the test_transform to the test_dataset\n",
        "test_dataset = test_dataset.dataset\n",
        "test_dataset.transform = test_transform\n",
        "\n",
        "# Create the DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUpiqEM82EFu",
        "outputId": "ebdff950-8f50-4278-ff04-6cd1556ce8d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet32(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock_32(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (1): BasicBlock_32(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock_32(\n",
              "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock_32(\n",
              "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock_32(\n",
              "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock_32(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=64, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the loss function, optimizer, and hyperparameters\n",
        "teacher = ResNet32()\n",
        "TA=ResNet20()\n",
        "#student = ResNet8()\n",
        "\n",
        "\"\"\"\n",
        "M=ResNet(block, num_blocks)\n",
        "TA=M.ResNet20()\n",
        "student=M.Resnet8()\n",
        "\"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load pre-trained teacher model weights\n",
        "#teacher.resnet50.load_state_dict(torch.load('/content/drive/MyDrive/resnet50_cifar10.pth'), False)\n",
        "teacher.load_state_dict(torch.load('/content/drive/MyDrive/Teacher32_Cifar100_final_weights.pth'), False)\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.SGD(TA.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "teacher.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ennU7wnL2RBr",
        "outputId": "3abbd6a5-f114-4b3f-90af-47f851371bec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/30], Train Loss: 5.1650, Train Acc: 7.09%, Test Loss: 4.1798, Test Acc: 10.02%\n",
            "Epoch [2/30], Train Loss: 3.3149, Train Acc: 15.63%, Test Loss: 3.5676, Test Acc: 18.78%\n",
            "Epoch [3/30], Train Loss: 2.3336, Train Acc: 23.71%, Test Loss: 3.1611, Test Acc: 24.09%\n",
            "Epoch [4/30], Train Loss: 1.7625, Train Acc: 29.60%, Test Loss: 2.6766, Test Acc: 31.52%\n",
            "Epoch [5/30], Train Loss: 1.4419, Train Acc: 33.68%, Test Loss: 2.7749, Test Acc: 32.14%\n",
            "Epoch [6/30], Train Loss: 1.2116, Train Acc: 36.93%, Test Loss: 2.3883, Test Acc: 37.73%\n",
            "Epoch [7/30], Train Loss: 1.0623, Train Acc: 39.16%, Test Loss: 2.3530, Test Acc: 38.73%\n",
            "Epoch [8/30], Train Loss: 0.9589, Train Acc: 40.79%, Test Loss: 2.2348, Test Acc: 40.67%\n",
            "Epoch [9/30], Train Loss: 0.8774, Train Acc: 41.88%, Test Loss: 2.1780, Test Acc: 41.91%\n",
            "Epoch [10/30], Train Loss: 0.8090, Train Acc: 43.30%, Test Loss: 2.1008, Test Acc: 43.52%\n",
            "Epoch [11/30], Train Loss: 0.7578, Train Acc: 44.17%, Test Loss: 2.1999, Test Acc: 41.60%\n",
            "Epoch [12/30], Train Loss: 0.7239, Train Acc: 44.48%, Test Loss: 2.0755, Test Acc: 44.05%\n",
            "Epoch [13/30], Train Loss: 0.6878, Train Acc: 45.34%, Test Loss: 2.0733, Test Acc: 44.04%\n",
            "Epoch [14/30], Train Loss: 0.6584, Train Acc: 45.98%, Test Loss: 2.0085, Test Acc: 45.91%\n",
            "Epoch [15/30], Train Loss: 0.6362, Train Acc: 46.25%, Test Loss: 2.0593, Test Acc: 45.10%\n",
            "Epoch [16/30], Train Loss: 0.6104, Train Acc: 46.65%, Test Loss: 2.1370, Test Acc: 43.83%\n",
            "Epoch [17/30], Train Loss: 0.5901, Train Acc: 47.12%, Test Loss: 1.9796, Test Acc: 46.69%\n",
            "Epoch [18/30], Train Loss: 0.5742, Train Acc: 47.47%, Test Loss: 1.9586, Test Acc: 46.68%\n",
            "Epoch [19/30], Train Loss: 0.5524, Train Acc: 47.84%, Test Loss: 1.9019, Test Acc: 47.70%\n",
            "Epoch [20/30], Train Loss: 0.5423, Train Acc: 48.09%, Test Loss: 1.8969, Test Acc: 48.07%\n",
            "Epoch [21/30], Train Loss: 0.5328, Train Acc: 48.34%, Test Loss: 1.9743, Test Acc: 46.72%\n",
            "Epoch [22/30], Train Loss: 0.5253, Train Acc: 48.16%, Test Loss: 1.8920, Test Acc: 48.10%\n",
            "Epoch [23/30], Train Loss: 0.5060, Train Acc: 48.78%, Test Loss: 1.9547, Test Acc: 47.34%\n",
            "Epoch [24/30], Train Loss: 0.4990, Train Acc: 48.86%, Test Loss: 1.9169, Test Acc: 47.79%\n",
            "Epoch [25/30], Train Loss: 0.4872, Train Acc: 48.92%, Test Loss: 1.9040, Test Acc: 47.99%\n",
            "Epoch [26/30], Train Loss: 0.4811, Train Acc: 49.08%, Test Loss: 1.8854, Test Acc: 48.65%\n",
            "Epoch [27/30], Train Loss: 0.4824, Train Acc: 49.36%, Test Loss: 1.9345, Test Acc: 47.00%\n",
            "Epoch [28/30], Train Loss: 0.4732, Train Acc: 49.50%, Test Loss: 1.8881, Test Acc: 48.10%\n",
            "Epoch [29/30], Train Loss: 0.4674, Train Acc: 49.27%, Test Loss: 1.8898, Test Acc: 48.06%\n",
            "Epoch [30/30], Train Loss: 0.4634, Train Acc: 49.58%, Test Loss: 1.8929, Test Acc: 48.12%\n"
          ]
        }
      ],
      "source": [
        "#train TA model, knowledge distilation from teacher model\n",
        "\n",
        "\n",
        "temperature = 3.3\n",
        "num_epochs = 30\n",
        "\n",
        "# Train the student model using knowledge distillation\n",
        "\n",
        "\n",
        "teacher.eval()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    TA.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for teacher and student models\n",
        "        with torch.no_grad():\n",
        "            teacher_output = teacher(images)\n",
        "        TA_output = TA(images)\n",
        "\n",
        "        # Apply temperature scaling to logits\n",
        "        teacher_output = teacher_output / temperature\n",
        "        TA_output = TA_output / temperature\n",
        "\n",
        "        # Calculate the distillation loss\n",
        "        loss = criterion(nn.functional.log_softmax(TA_output, dim=1), nn.functional.softmax(teacher_output, dim=1))*(temperature**2)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(TA_output.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate the student model on the test set\n",
        "    TA.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            TA_output = TA(images)\n",
        "            loss = nn.functional.cross_entropy(TA_output, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(TA_output.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print epoch number, training loss, training accuracy, test loss, and test accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, train_loss/len(train_dataset), 100*train_correct/len(train_dataset),\n",
        "                  test_loss/len(test_dataset), 100*test_correct/len(test_dataset)))\n",
        "\n",
        "# Save the student model weights\n",
        "torch.save(TA.state_dict(), '/content/drive/MyDrive/TA_ResNet20_Cifar100_weights.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNbET1KGB47y",
        "outputId": "02f724c7-1030-433a-9249-9a18a8e1f213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#without replacing the fc of student\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "TA.load_state_dict(torch.load('/content/drive/MyDrive/TA_ResNet20_Cifar100_weights.pth'), False)\n",
        "TA.eval()\n",
        "\n",
        "\n",
        "class ResNet8(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ResNet8, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(BasicBlock, 16, 1, stride=1)\n",
        "        self.layer2 = self.make_layer(BasicBlock, 32, 1, stride=2)\n",
        "        self.layer3 = self.make_layer(BasicBlock, 64, 1, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "student = ResNet8()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vjvs0mjcrsi",
        "outputId": "f1a225ef-ca7a-4beb-f2ff-7791426cd658"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/40], Train Loss: 4.2591, Train Acc: 8.58%, Test Loss: 4.1141, Test Acc: 12.68%\n",
            "Epoch [2/40], Train Loss: 2.4071, Train Acc: 18.68%, Test Loss: 3.4664, Test Acc: 21.95%\n",
            "Epoch [3/40], Train Loss: 1.6769, Train Acc: 25.21%, Test Loss: 3.0782, Test Acc: 25.99%\n",
            "Epoch [4/40], Train Loss: 1.3363, Train Acc: 29.04%, Test Loss: 3.1960, Test Acc: 25.75%\n",
            "Epoch [5/40], Train Loss: 1.1400, Train Acc: 31.77%, Test Loss: 3.1586, Test Acc: 27.43%\n",
            "Epoch [6/40], Train Loss: 1.0293, Train Acc: 33.19%, Test Loss: 2.5180, Test Acc: 34.88%\n",
            "Epoch [7/40], Train Loss: 0.9450, Train Acc: 34.64%, Test Loss: 2.7616, Test Acc: 32.01%\n",
            "Epoch [8/40], Train Loss: 0.8962, Train Acc: 35.01%, Test Loss: 2.4855, Test Acc: 34.84%\n",
            "Epoch [9/40], Train Loss: 0.8583, Train Acc: 35.74%, Test Loss: 2.6226, Test Acc: 33.80%\n",
            "Epoch [10/40], Train Loss: 0.8274, Train Acc: 36.40%, Test Loss: 2.9713, Test Acc: 31.56%\n",
            "Epoch [11/40], Train Loss: 0.8120, Train Acc: 36.37%, Test Loss: 2.8304, Test Acc: 32.47%\n",
            "Epoch [12/40], Train Loss: 0.8010, Train Acc: 36.71%, Test Loss: 2.4742, Test Acc: 36.62%\n",
            "Epoch [13/40], Train Loss: 0.7826, Train Acc: 36.90%, Test Loss: 2.6283, Test Acc: 34.26%\n",
            "Epoch [14/40], Train Loss: 0.7785, Train Acc: 37.09%, Test Loss: 2.4432, Test Acc: 36.68%\n",
            "Epoch [15/40], Train Loss: 0.7555, Train Acc: 37.38%, Test Loss: 2.4130, Test Acc: 35.63%\n",
            "Epoch [16/40], Train Loss: 0.7564, Train Acc: 37.23%, Test Loss: 2.5739, Test Acc: 34.44%\n",
            "Epoch [17/40], Train Loss: 0.7427, Train Acc: 37.50%, Test Loss: 2.8505, Test Acc: 31.71%\n",
            "Epoch [18/40], Train Loss: 0.7354, Train Acc: 37.53%, Test Loss: 2.8717, Test Acc: 32.55%\n",
            "Epoch [19/40], Train Loss: 0.7356, Train Acc: 37.86%, Test Loss: 2.9639, Test Acc: 31.72%\n",
            "Epoch [20/40], Train Loss: 0.7354, Train Acc: 37.52%, Test Loss: 2.6945, Test Acc: 33.95%\n",
            "Epoch [21/40], Train Loss: 0.7180, Train Acc: 37.83%, Test Loss: 2.6068, Test Acc: 33.80%\n",
            "Epoch [22/40], Train Loss: 0.7088, Train Acc: 37.94%, Test Loss: 3.0882, Test Acc: 29.38%\n",
            "Epoch [23/40], Train Loss: 0.7255, Train Acc: 37.91%, Test Loss: 2.7540, Test Acc: 33.95%\n",
            "Epoch [24/40], Train Loss: 0.7051, Train Acc: 37.98%, Test Loss: 2.4781, Test Acc: 36.30%\n",
            "Epoch [25/40], Train Loss: 0.7030, Train Acc: 38.13%, Test Loss: 2.5497, Test Acc: 35.53%\n",
            "Epoch [26/40], Train Loss: 0.6991, Train Acc: 37.94%, Test Loss: 2.7168, Test Acc: 34.48%\n",
            "Epoch [27/40], Train Loss: 0.6969, Train Acc: 38.36%, Test Loss: 2.5806, Test Acc: 34.58%\n",
            "Epoch [28/40], Train Loss: 0.7022, Train Acc: 38.14%, Test Loss: 2.8134, Test Acc: 32.81%\n",
            "Epoch [29/40], Train Loss: 0.7000, Train Acc: 38.04%, Test Loss: 2.9549, Test Acc: 31.88%\n",
            "Epoch [30/40], Train Loss: 0.6969, Train Acc: 38.23%, Test Loss: 2.6289, Test Acc: 35.17%\n",
            "Epoch [31/40], Train Loss: 0.5005, Train Acc: 40.46%, Test Loss: 2.2078, Test Acc: 41.07%\n",
            "Epoch [32/40], Train Loss: 0.4693, Train Acc: 40.96%, Test Loss: 2.2089, Test Acc: 41.09%\n",
            "Epoch [33/40], Train Loss: 0.4621, Train Acc: 41.03%, Test Loss: 2.1930, Test Acc: 41.51%\n",
            "Epoch [34/40], Train Loss: 0.4509, Train Acc: 41.15%, Test Loss: 2.1747, Test Acc: 41.58%\n",
            "Epoch [35/40], Train Loss: 0.4512, Train Acc: 41.23%, Test Loss: 2.1616, Test Acc: 41.91%\n",
            "Epoch [36/40], Train Loss: 0.4481, Train Acc: 41.29%, Test Loss: 2.1452, Test Acc: 42.05%\n",
            "Epoch [37/40], Train Loss: 0.4471, Train Acc: 41.38%, Test Loss: 2.1668, Test Acc: 41.84%\n",
            "Epoch [38/40], Train Loss: 0.4459, Train Acc: 41.54%, Test Loss: 2.1660, Test Acc: 41.84%\n",
            "Epoch [39/40], Train Loss: 0.4431, Train Acc: 41.37%, Test Loss: 2.1200, Test Acc: 42.40%\n",
            "Epoch [40/40], Train Loss: 0.4379, Train Acc: 41.52%, Test Loss: 2.1767, Test Acc: 41.62%\n"
          ]
        }
      ],
      "source": [
        "#Without replacing fc of student, train for loss*temperature**2\n",
        "\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "temperature = 3.3\n",
        "num_epochs = 40\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "           TA_output = TA(images)\n",
        "        student_output = student(images)\n",
        "\n",
        "        TA_output = TA_output / temperature\n",
        "        student_output = student_output / temperature\n",
        "\n",
        "        loss = criterion(nn.functional.log_softmax(student_output, dim=1), nn.functional.softmax(TA_output, dim=1))*(temperature**2)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(student_output.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "    student.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            student_output = student(images)\n",
        "            loss = nn.functional.cross_entropy(student_output, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(student_output.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print epoch number, training loss, training accuracy, test loss, and test accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, train_loss/len(train_dataset), 100*train_correct/len(train_dataset),\n",
        "                  test_loss/len(test_dataset), 100*test_correct/len(test_dataset)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the student model weights\n",
        "torch.save(student.state_dict(), '/content/drive/MyDrive/student_cifar100_fc_weights.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Y9lrvN3c4O",
        "outputId": "244eff45-9cdb-43e2-8683-77a803e30f1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/40], Train Loss: 0.4853, Train Acc: 4.65%, Test Loss: 4.2361, Test Acc: 6.71%\n",
            "Epoch [2/40], Train Loss: 0.3484, Train Acc: 8.46%, Test Loss: 3.9424, Test Acc: 10.57%\n",
            "Epoch [3/40], Train Loss: 0.2836, Train Acc: 11.89%, Test Loss: 3.6119, Test Acc: 13.28%\n",
            "Epoch [4/40], Train Loss: 0.2421, Train Acc: 14.61%, Test Loss: 3.6146, Test Acc: 15.13%\n",
            "Epoch [5/40], Train Loss: 0.2108, Train Acc: 17.31%, Test Loss: 3.8683, Test Acc: 15.54%\n",
            "Epoch [6/40], Train Loss: 0.1850, Train Acc: 19.72%, Test Loss: 3.2256, Test Acc: 20.05%\n",
            "Epoch [7/40], Train Loss: 0.1671, Train Acc: 21.62%, Test Loss: 3.3436, Test Acc: 20.65%\n",
            "Epoch [8/40], Train Loss: 0.1524, Train Acc: 23.16%, Test Loss: 3.1655, Test Acc: 21.39%\n",
            "Epoch [9/40], Train Loss: 0.1432, Train Acc: 24.54%, Test Loss: 3.3051, Test Acc: 22.16%\n",
            "Epoch [10/40], Train Loss: 0.1360, Train Acc: 25.56%, Test Loss: 3.1325, Test Acc: 23.95%\n",
            "Epoch [11/40], Train Loss: 0.1294, Train Acc: 26.22%, Test Loss: 3.0319, Test Acc: 24.38%\n",
            "Epoch [12/40], Train Loss: 0.1253, Train Acc: 26.53%, Test Loss: 3.0573, Test Acc: 24.72%\n",
            "Epoch [13/40], Train Loss: 0.1212, Train Acc: 27.28%, Test Loss: 3.6263, Test Acc: 21.00%\n",
            "Epoch [14/40], Train Loss: 0.1184, Train Acc: 27.52%, Test Loss: 2.8785, Test Acc: 26.71%\n",
            "Epoch [15/40], Train Loss: 0.1154, Train Acc: 27.87%, Test Loss: 3.3095, Test Acc: 22.94%\n",
            "Epoch [16/40], Train Loss: 0.1141, Train Acc: 28.04%, Test Loss: 2.9993, Test Acc: 25.46%\n",
            "Epoch [17/40], Train Loss: 0.1130, Train Acc: 28.41%, Test Loss: 3.0945, Test Acc: 23.47%\n",
            "Epoch [18/40], Train Loss: 0.1087, Train Acc: 28.50%, Test Loss: 3.0194, Test Acc: 26.94%\n",
            "Epoch [19/40], Train Loss: 0.1085, Train Acc: 28.80%, Test Loss: 3.7404, Test Acc: 19.45%\n",
            "Epoch [20/40], Train Loss: 0.1088, Train Acc: 28.74%, Test Loss: 2.8976, Test Acc: 26.94%\n",
            "Epoch [21/40], Train Loss: 0.1065, Train Acc: 29.07%, Test Loss: 3.1578, Test Acc: 25.46%\n",
            "Epoch [22/40], Train Loss: 0.1047, Train Acc: 29.30%, Test Loss: 3.1275, Test Acc: 25.68%\n",
            "Epoch [23/40], Train Loss: 0.1040, Train Acc: 29.36%, Test Loss: 3.0035, Test Acc: 26.06%\n",
            "Epoch [24/40], Train Loss: 0.1046, Train Acc: 29.32%, Test Loss: 2.7610, Test Acc: 28.80%\n",
            "Epoch [25/40], Train Loss: 0.1036, Train Acc: 29.68%, Test Loss: 3.0720, Test Acc: 26.47%\n",
            "Epoch [26/40], Train Loss: 0.1017, Train Acc: 29.75%, Test Loss: 2.9036, Test Acc: 28.03%\n",
            "Epoch [27/40], Train Loss: 0.1011, Train Acc: 29.96%, Test Loss: 3.3128, Test Acc: 23.55%\n",
            "Epoch [28/40], Train Loss: 0.0999, Train Acc: 29.96%, Test Loss: 2.7570, Test Acc: 29.09%\n",
            "Epoch [29/40], Train Loss: 0.1007, Train Acc: 29.82%, Test Loss: 2.7904, Test Acc: 28.68%\n",
            "Epoch [30/40], Train Loss: 0.1003, Train Acc: 29.97%, Test Loss: 3.1692, Test Acc: 25.60%\n",
            "Epoch [31/40], Train Loss: 0.0808, Train Acc: 31.77%, Test Loss: 2.6122, Test Acc: 32.03%\n",
            "Epoch [32/40], Train Loss: 0.0772, Train Acc: 32.44%, Test Loss: 2.5374, Test Acc: 32.88%\n",
            "Epoch [33/40], Train Loss: 0.0759, Train Acc: 32.60%, Test Loss: 2.5011, Test Acc: 33.30%\n",
            "Epoch [34/40], Train Loss: 0.0753, Train Acc: 32.61%, Test Loss: 2.5123, Test Acc: 33.08%\n",
            "Epoch [35/40], Train Loss: 0.0751, Train Acc: 32.68%, Test Loss: 2.4933, Test Acc: 33.39%\n",
            "Epoch [36/40], Train Loss: 0.0747, Train Acc: 32.85%, Test Loss: 2.5231, Test Acc: 33.07%\n",
            "Epoch [37/40], Train Loss: 0.0743, Train Acc: 32.92%, Test Loss: 2.5500, Test Acc: 32.53%\n",
            "Epoch [38/40], Train Loss: 0.0738, Train Acc: 32.79%, Test Loss: 2.5459, Test Acc: 32.83%\n",
            "Epoch [39/40], Train Loss: 0.0742, Train Acc: 32.92%, Test Loss: 2.5497, Test Acc: 32.65%\n",
            "Epoch [40/40], Train Loss: 0.0737, Train Acc: 32.92%, Test Loss: 2.5136, Test Acc: 33.18%\n"
          ]
        }
      ],
      "source": [
        "#without multiplying loss*temperature**2 and without replacing student fc as well\n",
        "\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "temperature = 3.3\n",
        "num_epochs = 40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for teacher and student models\n",
        "        with torch.no_grad():\n",
        "           TA_output = TA(images)\n",
        "        student_output = student(images)\n",
        "\n",
        "        # Apply temperature scaling to logits\n",
        "        TA_output = TA_output / temperature\n",
        "        student_output = student_output / temperature\n",
        "\n",
        "        loss = criterion(nn.functional.log_softmax(student_output, dim=1), nn.functional.softmax(TA_output, dim=1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(student_output.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    student.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            student_output = student(images)\n",
        "            loss = nn.functional.cross_entropy(student_output, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(student_output.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, train_loss/len(train_dataset), 100*train_correct/len(train_dataset),\n",
        "                  test_loss/len(test_dataset), 100*test_correct/len(test_dataset)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the student model weights\n",
        "torch.save(student.state_dict(), '/content/drive/MyDrive/student_cifar100_fc_weights.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_u8t1xsXLhl",
        "outputId": "44699ab1-db41-4896-f528-7e0233420d4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#Replace the student's fully connected network by teacher's liniar layer(64,100)\n",
        "\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "# Load trained TA model weights\n",
        "TA.load_state_dict(torch.load('/content/drive/MyDrive/TA_ResNet20_Cifar100_weights.pth'), False)\n",
        "\n",
        "TA.eval()\n",
        "\n",
        "\n",
        "class ResNet8(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ResNet8, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(BasicBlock, 16, 1, stride=1)\n",
        "        self.layer2 = self.make_layer(BasicBlock, 32, 1, stride=2)\n",
        "        self.layer3 = self.make_layer(BasicBlock, 64, 1, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "\n",
        "        student_fc_layer=nn.Linear(64, 100)\n",
        "        student_fc_layer.weight.data = teacher.linear.weight.data\n",
        "        student_fc_layer.bias.data = teacher.linear.bias.data\n",
        "        # Replace\n",
        "        self.fc = student_fc_layer\n",
        "\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "student = ResNet8()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdovJdtrXB63",
        "outputId": "1f4db553-cd27-4ab0-e776-5ba0e9753aa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/40], Train Loss: 0.3581, Train Acc: 10.91%, Test Loss: 4.3687, Test Acc: 12.32%\n",
            "Epoch [2/40], Train Loss: 0.2344, Train Acc: 18.12%, Test Loss: 3.5091, Test Acc: 18.72%\n",
            "Epoch [3/40], Train Loss: 0.1815, Train Acc: 22.57%, Test Loss: 3.6278, Test Acc: 20.30%\n",
            "Epoch [4/40], Train Loss: 0.1538, Train Acc: 25.19%, Test Loss: 3.3707, Test Acc: 21.87%\n",
            "Epoch [5/40], Train Loss: 0.1365, Train Acc: 26.64%, Test Loss: 3.0284, Test Acc: 25.64%\n",
            "Epoch [6/40], Train Loss: 0.1272, Train Acc: 27.66%, Test Loss: 3.1678, Test Acc: 24.59%\n",
            "Epoch [7/40], Train Loss: 0.1209, Train Acc: 28.35%, Test Loss: 3.0752, Test Acc: 26.12%\n",
            "Epoch [8/40], Train Loss: 0.1181, Train Acc: 28.52%, Test Loss: 3.1218, Test Acc: 25.65%\n",
            "Epoch [9/40], Train Loss: 0.1146, Train Acc: 28.56%, Test Loss: 3.1547, Test Acc: 26.03%\n",
            "Epoch [10/40], Train Loss: 0.1123, Train Acc: 29.05%, Test Loss: 3.3496, Test Acc: 22.77%\n",
            "Epoch [11/40], Train Loss: 0.1112, Train Acc: 29.06%, Test Loss: 3.1527, Test Acc: 23.44%\n",
            "Epoch [12/40], Train Loss: 0.1083, Train Acc: 29.35%, Test Loss: 2.8254, Test Acc: 27.73%\n",
            "Epoch [13/40], Train Loss: 0.1059, Train Acc: 29.67%, Test Loss: 2.7183, Test Acc: 29.30%\n",
            "Epoch [14/40], Train Loss: 0.1057, Train Acc: 29.49%, Test Loss: 2.8626, Test Acc: 28.66%\n",
            "Epoch [15/40], Train Loss: 0.1044, Train Acc: 29.68%, Test Loss: 2.7396, Test Acc: 29.18%\n",
            "Epoch [16/40], Train Loss: 0.1034, Train Acc: 29.76%, Test Loss: 3.1009, Test Acc: 25.85%\n",
            "Epoch [17/40], Train Loss: 0.1030, Train Acc: 29.86%, Test Loss: 3.6264, Test Acc: 24.16%\n",
            "Epoch [18/40], Train Loss: 0.1001, Train Acc: 30.20%, Test Loss: 2.8761, Test Acc: 27.57%\n",
            "Epoch [19/40], Train Loss: 0.1013, Train Acc: 30.11%, Test Loss: 2.8751, Test Acc: 29.47%\n",
            "Epoch [20/40], Train Loss: 0.0996, Train Acc: 30.20%, Test Loss: 2.7058, Test Acc: 29.67%\n",
            "Epoch [21/40], Train Loss: 0.0991, Train Acc: 30.21%, Test Loss: 3.0531, Test Acc: 26.68%\n",
            "Epoch [22/40], Train Loss: 0.0983, Train Acc: 30.68%, Test Loss: 3.1130, Test Acc: 26.67%\n",
            "Epoch [23/40], Train Loss: 0.0982, Train Acc: 30.64%, Test Loss: 2.9169, Test Acc: 26.98%\n",
            "Epoch [24/40], Train Loss: 0.0976, Train Acc: 30.71%, Test Loss: 3.4053, Test Acc: 24.42%\n",
            "Epoch [25/40], Train Loss: 0.0970, Train Acc: 30.65%, Test Loss: 2.8547, Test Acc: 28.08%\n",
            "Epoch [26/40], Train Loss: 0.0968, Train Acc: 30.55%, Test Loss: 2.7725, Test Acc: 29.24%\n",
            "Epoch [27/40], Train Loss: 0.0968, Train Acc: 30.77%, Test Loss: 2.9255, Test Acc: 28.06%\n",
            "Epoch [28/40], Train Loss: 0.0962, Train Acc: 30.83%, Test Loss: 3.2398, Test Acc: 25.31%\n",
            "Epoch [29/40], Train Loss: 0.0954, Train Acc: 30.80%, Test Loss: 4.5070, Test Acc: 20.01%\n",
            "Epoch [30/40], Train Loss: 0.0952, Train Acc: 30.66%, Test Loss: 3.0167, Test Acc: 27.08%\n",
            "Epoch [31/40], Train Loss: 0.0761, Train Acc: 32.73%, Test Loss: 2.5042, Test Acc: 33.48%\n",
            "Epoch [32/40], Train Loss: 0.0734, Train Acc: 33.31%, Test Loss: 2.4902, Test Acc: 33.50%\n",
            "Epoch [33/40], Train Loss: 0.0722, Train Acc: 33.47%, Test Loss: 2.4580, Test Acc: 34.01%\n",
            "Epoch [34/40], Train Loss: 0.0718, Train Acc: 33.32%, Test Loss: 2.4972, Test Acc: 33.60%\n",
            "Epoch [35/40], Train Loss: 0.0713, Train Acc: 33.41%, Test Loss: 2.4468, Test Acc: 34.35%\n",
            "Epoch [36/40], Train Loss: 0.0711, Train Acc: 33.74%, Test Loss: 2.5055, Test Acc: 33.69%\n",
            "Epoch [37/40], Train Loss: 0.0709, Train Acc: 33.52%, Test Loss: 2.4648, Test Acc: 33.86%\n",
            "Epoch [38/40], Train Loss: 0.0708, Train Acc: 33.66%, Test Loss: 2.4761, Test Acc: 33.91%\n",
            "Epoch [39/40], Train Loss: 0.0706, Train Acc: 33.76%, Test Loss: 2.4610, Test Acc: 34.18%\n",
            "Epoch [40/40], Train Loss: 0.0705, Train Acc: 33.57%, Test Loss: 2.4860, Test Acc: 33.98%\n"
          ]
        }
      ],
      "source": [
        "# Train student after Replacing fc and without loss*temperature**2\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "temperature = 3.3\n",
        "num_epochs = 40\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for teacher and student models\n",
        "        with torch.no_grad():\n",
        "           TA_output = TA(images)\n",
        "        student_output = student(images)\n",
        "\n",
        "        # Apply temperature scaling to logits\n",
        "        TA_output = TA_output / temperature\n",
        "        student_output = student_output / temperature\n",
        "\n",
        "        # Calculate the distillation loss\n",
        "        loss = criterion(nn.functional.log_softmax(student_output, dim=1), nn.functional.softmax(TA_output, dim=1))\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(student_output.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate the student model on the test set\n",
        "    student.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            student_output = student(images)\n",
        "            loss = nn.functional.cross_entropy(student_output, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(student_output.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Print epoch number, training loss, training accuracy, test loss, and test accuracy\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, train_loss/len(train_dataset), 100*train_correct/len(train_dataset),\n",
        "                  test_loss/len(test_dataset), 100*test_correct/len(test_dataset)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the student model weights\n",
        "torch.save(student.state_dict(), '/content/drive/MyDrive/student_cifar100_fc_weights.pth')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1f509e-9a3a-44ae-85d3-003a0b6c0a0f",
        "id": "ZAb2AWYTwFQd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/40], Train Loss: 0.5111, Train Acc: 41.52%, Test Loss: 2.1876, Test Acc: 41.63%\n",
            "Epoch [2/40], Train Loss: 0.5315, Train Acc: 41.05%, Test Loss: 2.4883, Test Acc: 37.52%\n",
            "Epoch [3/40], Train Loss: 0.5670, Train Acc: 40.52%, Test Loss: 2.7210, Test Acc: 33.27%\n",
            "Epoch [4/40], Train Loss: 0.6028, Train Acc: 39.81%, Test Loss: 2.3628, Test Acc: 37.87%\n",
            "Epoch [5/40], Train Loss: 0.6140, Train Acc: 39.41%, Test Loss: 2.9370, Test Acc: 32.47%\n",
            "Epoch [6/40], Train Loss: 0.6327, Train Acc: 39.34%, Test Loss: 2.5183, Test Acc: 36.93%\n",
            "Epoch [7/40], Train Loss: 0.6384, Train Acc: 39.15%, Test Loss: 2.3909, Test Acc: 38.13%\n",
            "Epoch [8/40], Train Loss: 0.6432, Train Acc: 39.15%, Test Loss: 2.4354, Test Acc: 37.88%\n",
            "Epoch [9/40], Train Loss: 0.6482, Train Acc: 39.02%, Test Loss: 2.4595, Test Acc: 36.19%\n",
            "Epoch [10/40], Train Loss: 0.6559, Train Acc: 38.90%, Test Loss: 2.3300, Test Acc: 38.22%\n",
            "Epoch [11/40], Train Loss: 0.6482, Train Acc: 38.75%, Test Loss: 2.5488, Test Acc: 35.97%\n",
            "Epoch [12/40], Train Loss: 0.6454, Train Acc: 39.00%, Test Loss: 2.3976, Test Acc: 36.17%\n",
            "Epoch [13/40], Train Loss: 0.6497, Train Acc: 38.84%, Test Loss: 2.3468, Test Acc: 38.22%\n",
            "Epoch [14/40], Train Loss: 0.6574, Train Acc: 39.00%, Test Loss: 2.4934, Test Acc: 36.37%\n",
            "Epoch [15/40], Train Loss: 0.6567, Train Acc: 38.92%, Test Loss: 2.4849, Test Acc: 35.88%\n",
            "Epoch [16/40], Train Loss: 0.6453, Train Acc: 38.97%, Test Loss: 2.6503, Test Acc: 33.85%\n",
            "Epoch [17/40], Train Loss: 0.6559, Train Acc: 38.69%, Test Loss: 2.3774, Test Acc: 38.20%\n",
            "Epoch [18/40], Train Loss: 0.6520, Train Acc: 38.79%, Test Loss: 2.4501, Test Acc: 37.88%\n",
            "Epoch [19/40], Train Loss: 0.6526, Train Acc: 38.75%, Test Loss: 2.5866, Test Acc: 35.86%\n",
            "Epoch [20/40], Train Loss: 0.6540, Train Acc: 38.90%, Test Loss: 2.6746, Test Acc: 35.70%\n",
            "Epoch [21/40], Train Loss: 0.6429, Train Acc: 39.05%, Test Loss: 2.8683, Test Acc: 30.76%\n",
            "Epoch [22/40], Train Loss: 0.6440, Train Acc: 39.11%, Test Loss: 2.4242, Test Acc: 37.17%\n",
            "Epoch [23/40], Train Loss: 0.6454, Train Acc: 38.90%, Test Loss: 2.4509, Test Acc: 36.88%\n",
            "Epoch [24/40], Train Loss: 0.6510, Train Acc: 38.77%, Test Loss: 2.7876, Test Acc: 33.33%\n",
            "Epoch [25/40], Train Loss: 0.6580, Train Acc: 38.62%, Test Loss: 2.4344, Test Acc: 36.87%\n",
            "Epoch [26/40], Train Loss: 0.6501, Train Acc: 38.96%, Test Loss: 2.4071, Test Acc: 37.49%\n",
            "Epoch [27/40], Train Loss: 0.6431, Train Acc: 38.92%, Test Loss: 2.5598, Test Acc: 35.83%\n",
            "Epoch [28/40], Train Loss: 0.6432, Train Acc: 38.99%, Test Loss: 2.5766, Test Acc: 35.71%\n",
            "Epoch [29/40], Train Loss: 0.6534, Train Acc: 38.88%, Test Loss: 2.3456, Test Acc: 37.74%\n",
            "Epoch [30/40], Train Loss: 0.6405, Train Acc: 38.98%, Test Loss: 2.6731, Test Acc: 34.61%\n",
            "Epoch [31/40], Train Loss: 0.4697, Train Acc: 40.88%, Test Loss: 2.1623, Test Acc: 41.81%\n",
            "Epoch [32/40], Train Loss: 0.4392, Train Acc: 41.48%, Test Loss: 2.1398, Test Acc: 42.40%\n",
            "Epoch [33/40], Train Loss: 0.4273, Train Acc: 41.73%, Test Loss: 2.1177, Test Acc: 42.58%\n",
            "Epoch [34/40], Train Loss: 0.4253, Train Acc: 41.71%, Test Loss: 2.1730, Test Acc: 41.93%\n",
            "Epoch [35/40], Train Loss: 0.4215, Train Acc: 41.93%, Test Loss: 2.1589, Test Acc: 41.98%\n",
            "Epoch [36/40], Train Loss: 0.4174, Train Acc: 41.97%, Test Loss: 2.1585, Test Acc: 42.30%\n",
            "Epoch [37/40], Train Loss: 0.4142, Train Acc: 42.00%, Test Loss: 2.1936, Test Acc: 41.60%\n",
            "Epoch [38/40], Train Loss: 0.4131, Train Acc: 41.83%, Test Loss: 2.1245, Test Acc: 42.45%\n",
            "Epoch [39/40], Train Loss: 0.4112, Train Acc: 42.05%, Test Loss: 2.1768, Test Acc: 41.75%\n",
            "Epoch [40/40], Train Loss: 0.4081, Train Acc: 42.03%, Test Loss: 2.1533, Test Acc: 42.35%\n"
          ]
        }
      ],
      "source": [
        "#Train student for replacing fc and loss*temperature**2 with NLL (Negative Log Loss)\n",
        "\n",
        "criterion = nn.KLDivLoss(reduction='batchmean')\n",
        "optimizer = optim.SGD(student.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
        "\n",
        "temperature = 3.16\n",
        "num_epochs = 40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student.train()\n",
        "    train_loss = 0.0\n",
        "    train_correct = 0\n",
        "    alpha = 0.5\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass for teacher and student models\n",
        "        with torch.no_grad():\n",
        "           TA_output = TA(images)\n",
        "        student_output = student(images)\n",
        "\n",
        "        # Apply temperature scaling to logits\n",
        "        TA_output = TA_output / temperature\n",
        "        student_output = student_output / temperature\n",
        "\n",
        "        # Calculate the distillation loss\n",
        "        distillation_loss = criterion(nn.functional.log_softmax(student_output, dim=1), nn.functional.softmax(TA_output, dim=1))*(temperature**2)\n",
        "\n",
        "        # Calculate the negative log likelihood loss\n",
        "        nll_loss = nn.functional.nll_loss(nn.functional.log_softmax(TA_output, dim=1), labels)\n",
        "\n",
        "\n",
        "        # Combine the two losses using the weight alpha\n",
        "        loss = alpha * nll_loss + (1 - alpha) * distillation_loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate training loss and accuracy\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(student_output.data, 1)\n",
        "        train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Evaluate the student model on the test set\n",
        "    student.eval()\n",
        "    test_loss = 0.0\n",
        "    test_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            student_output = student(images)\n",
        "            loss = nn.functional.cross_entropy(student_output, labels)\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(student_output.data, 1)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Epoch [{}/{}], Train Loss: {:.4f}, Train Acc: {:.2f}%, Test Loss: {:.4f}, Test Acc: {:.2f}%'\n",
        "          .format(epoch+1, num_epochs, train_loss/len(train_dataset), 100*train_correct/len(train_dataset),\n",
        "                  test_loss/len(test_dataset), 100*test_correct/len(test_dataset)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}