# Knowledge-Distillation-With-Intermedite-Neural-Network

Abstract

Deep neural networks are effective models that perform well on a variety of tasks, but they are too big to be used on devices like smartphones or smaller embedded systems. By compressing a potent but heavy pre-trained neural network (teacher) into a lightweight neural network (student), knowledge distillation seeks consistent performance, where the goal is to have the student model mimic the behavior and predictions of the teacher model. Over the years, there have been several approaches to improve the distillation of knowledge from the large teacher model to the smaller student model, such as self-distillation, ensemble distillation, attention transfer, teacher-assistant distillation, etc. In this paper, we show that if the gap between the teacher model and the student model is too large, the performance of knowledge distillation falls. Thus, we engage an intermediate neural network (teacher-assistant/TA) to bridge the gap between the teacher and the student model and reuse the teacher model’s fully-connected layer in the student model. Moreover, we study the effects of using various teacher and teacher-assistant models for knowledge distillation on a fixed student model, both with and without reusing the teacher’s fully connected layers through experiments on the CIFAR-10 and CIFAR-100 datasets using various ResNet architectures.
